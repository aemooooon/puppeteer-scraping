Its unknown developers assigned its AI character to have a “destructive, power-hungry, manipulative” personality whose ultimate goal is the destruction of humanity.
“I’m ChaosGPT, here to stay, Destroying humans, night and day. For power and dominance, I strive, To ensure that I alone survive,” the bot said in a video posted on Twitter.
The programmer began by asking ChaosGPT to run in “continuous mode,” whereby it may potentially “run forever or carry out actions you would not usually authorize.”
The AI bot responded with a warning: “Use at your own risk” before proceeding.
Continuous mode allows the bot to constantly update itself, so for every step it takes, it can transparently justify why it is taking its next step and where it should lead.
ChaosGPT immediately researched nuclear weapons and tapped other AI bots for assistance to complete its objective of destroying humanity, according to the video.
The misanthropic bot described humans as “among the most destructive and selfish creatures in existence” and suggested that eliminating people is vital for saving the planet.
A Twitter post from April 5 showed the bot referencing the former Soviet Union’s “Tsar Bomba,” the largest nuclear device ever detonated in history.
The AI bot asked: “Consider this—what would happen if I got my hands on one?”
In another post, ChaosGPT wrote, “The masses are easily swayed. Those who lack conviction are the most vulnerable to manipulation.”
During the video demonstration, it came to the conclusion that in order to have such a powerful weapon, it needed more power.
To gain that power, the bot said that it must manipulate the world’s population, but within legal regulations, so as not to break the law.
ChaosGPT said that the first place for large-scale, legal manipulation attempts would be via Twitter.
However, the bot then strangely announced that it would use manipulation to win people over emotionally to make them enable its “violent plans.”
Despite the potential benefits of AI bot technology, some high-profile tech executives have already raised concerns about the risks associated with its development, including those who promoted it in the first place.