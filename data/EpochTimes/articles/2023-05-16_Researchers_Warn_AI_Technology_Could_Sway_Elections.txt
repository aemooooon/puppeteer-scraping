Researchers are concerned that advancements in new artificial intelligence (AI) applications could be used to mislead the voting public and potentially sway the outcomes of elections.
Recent advancements have allowed AI to respond to writing prompts in real-time, effectively mimicking human conversation. AI has also enabled people to create lifelike images, videos, and voiceovers, commonly referred to as deep fakes. As time progresses, these tools have only become more convincing, and some researchers now believe they could be used at scale in the upcoming 2024 U.S. election cycle.
While generative AI may help campaigns to rapidly produce targeted campaign emails, texts, and videos, they could also be used for more deceptive purposes like impersonating specific candidates.
Some scenarios researchers have considered include automated robocalls in a specific candidate’s voice, telling their supporters to get out to vote on the wrong date. Researchers have envisioned other potentially problematic scenarios, such as a faked newspaper headline informing readers that a particular candidate had dropped out of the race or faked audio impersonating an influential public figure.
“What if Elon Musk personally calls you and tells you to vote for a certain candidate?” said Oren Etzioni, the founding CEO of the Allen Institute for AI. “A lot of people would listen. But it’s not him.”
Deceptive actors could also produce relatively convincing faked audio impersonating a candidate as they confess to a crime or express a view that might alienate voters. On the inverse, a candidate might be able to plausibly deny an incriminating authentic recording by claiming it was faked. In any of the cases, the public could struggle to discern whether they’re getting the truth or a carefully constructed lie.
While the RNC acknowledged its use of AI-generated images for the ad, cybersecurity analyst Petko Stoyanov warned that nefarious actors and hostile nation-states won’t be so forthcoming in the future.
“What happens if an international entity—a cybercriminal or a nation state—impersonates someone. What is the impact? Do we have any recourse?” Stoyanov said. “We’re going to see a lot more misinformation from international sources.”
Though many researchers see the potential nefarious misuses of AI, campaigns are leaning into the tools as a way to communicate more effectively with voters.
Mike Nellis, CEO of progressive digital agency Authentic, said he encourages his team to use AI text generator ChatGPT to help write campaign ads. Nellis said that as long as his staff reviews the content ChatGPT produces before they disseminate it, he doesn’t see a problem.
Nellis’s team is currently working with Higher Ground Labs, to develop an AI tool called Quiller that can generate, distribute, and evaluate the effectiveness of new campaign fundraising emails.
Last month the the Civil Rights Division of the U.S. Department of Justice, the Consumer Financial Protection Bureau, the Federal Trade Commission (FTC), and the U.S. Equal Employment Opportunity Commission issued a joint statement warning that AI tools could be used to “perpetuate unlawful bias, automate unlawful discrimination, and produce other harmful outcomes” in the work force. FTC Chair Lina M. Khan said AI tools could “turbocharge fraud and automate discrimination.”
While policymakers are looking to constrain AI, some researchers have warned against overregulating the technology.
Jake Morabito, director of the Communications and Technology Task Force at the American Legislative Exchange Council, has warned that overregulation could stifle innovative AI technologies in their infancy.