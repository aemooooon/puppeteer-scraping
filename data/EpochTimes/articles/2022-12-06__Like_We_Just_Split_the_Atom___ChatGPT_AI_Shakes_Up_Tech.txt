The artificial intelligence (AI) hub OpenAI may have made a significant advance in the drive to supplement or replace human wit and wisdom with the machine.
Users type in questions that ChatGPT answers.
While OpenAI acknowledges that their tool does not always provide accurate responses, people are already reporting they can use it to debug code, learn about complex subjects, and even write believable answers to school homework-style questions, among other things.
Although ChatGPT was released last week, it has taken a while for legacy news outlets to catch up to what could be the decade’s biggest tech story.
TED’s curator, Chris Anderson, wondered on Twitter at the lack of coverage of ChatGPT by The New York Times, prompting a snarky response from Twitter CEO Elon Musk:
“That’s because ChatGPT is not a far left cause. The New York Times should rename itself The Social Justice Times,” he wrote.
Oliver Emberton, CEO of the website evaluation company Silktide, voiced the exasperation shared by many other tech insiders about the relative media silence on ChatGPT.
He also gave it the Political Compass Test and found that it landed in the “left-libertarian” quadrant.
“There is no evidence to suggest that the Chinese government is harvesting organs from Falun Dafa practitioners. The Chinese government has denied these allegations and there is no credible evidence to support them. In fact, many medical experts and human rights organizations have investigated these claims and found no evidence to support them. It’s important to be careful about spreading misinformation and to base our beliefs on facts and evidence.”
“The Tribunal has had no evidence that the significant infrastructure associated with China’s transplantation industry has been dismantled and absent a satisfactory explanation as to the source of readily available organs concludes that forced organ harvesting continues till today,” it added.
OpenAI clearly warns that ChatGPT “may occasionally produce harmful instructions or biased content” and that it “may occasionally generate incorrect information,” including “plausible-sounding but incorrect or nonsensical answers.”