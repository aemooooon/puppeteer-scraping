Generative artificial intelligence (AI) is becoming more common, including in politics. Images of Donald Trump getting violently arrested in New York or of the Pope wearing a puffy white coat are deepfakes that many believed, or wanted to believe, when they went viral.
Schmidt noted that social media companies like Alphabet, Twitter, and Meta cut thousands of jobs devoted to content moderation. Those roles might have helped ferret out AI deepfakes in the 2024 elections.
There is already plenty of such fake electioneering.
With AI, technologists on political payrolls can produce hyperreal color-enhanced images that play to public fears. If the voters weren’t scared before, they would be now and more likely to vote for the politician who best caters to fear by individualizing the messaging through bespoke microtargeting.
The risk is that electioneering—including by authoritarians who use deepfakes in fake elections to legitimize their power—becomes yet more untethered from reality. Authoritarian politics is already devoid of reality because of censorship and disinformation spoon-fed to their populations, as well as underlings who, in turn, feed dictators in countries like Russia and China with what they want to hear. With AI, authoritarians will have yet more security of tenure despite their rosier-than-justified view of their own power.
Democracies are at least a little better. Voters provide leaders with some semblance of reality through open contestation and the wisdom of crowds. Democracies become demagoguery, however, when those crowds are manipulated with too much false information, either of the too-rosy or too-dire kind.
AI hands the technology necessary for demagoguery to the demagogue, by making deepfakes of whatever type—imagery, text, video, and voice—look and sound so real as to be believable.
“In Chicago, the runner-up in the mayoral vote in April complained that a Twitter account masquerading as a news outlet had used A.I. to clone his voice in a way that suggested he condoned police brutality,” write Tiffany Hsu and Steven Lee Myers in The New York Times.
Shane Goldmacher, also in the NY Times, writes that “Republican and Democratic engineers alike are racing to develop tools to harness A.I. to make advertising more efficient, to engage in predictive analysis of public behavior, to write more and more personalized copy and to discover new patterns in mountains of voter data.”
In political experiments run by the Democratic National Committee, AI-generated texts perform at least as well as those generated by humans. Higher Ground Labs, which invests in technologies to support progressive politics, has an AI system called Quiller that simultaneously writes, sends, and tests fundraising emails.
Goldmacher interviewed political operatives with concerns that “bad actors” could use AI to waste opposing-campaign staff time by taking on the persona of potential voters, producing deepfakes of their candidate providing personalized videos to supporters, or faking voice messages by the opposing candidate for delivery to voters the day before the election.
Unless the political use of AI is regulated, the technology could become so ubiquitous and powerful as to compromise the informed electorate upon which real democracy depends. At that point, elected officials will be dependent upon AI for their own success and unlikely to change overly-permissive election laws that got them elected in the first place. Why bite the AI hand that feeds them? At that point, we could be stuck with AI politics forever, just as we are with overly-permissive campaign finance laws.
Some political experts and election consultants are now calling for the imposition of regulations to stop the AI generation of synthetic images for political ads.
OpenAI, the creator of ChatGPT, bans the generation of high-volume campaign materials.
However, this self-regulation is less helpful than it sounds. Contrary to its name, OpenAI does not use open-source code. There are numerous open-source alternatives that any political campaign—or terrorist organization, for that matter—can download and alter to their purposes.