Apple announced Thursday it is planning to scan all iPhones in the United States for child abuse imagery, raising alarm among security experts who said the plan could allow the firm to surveil tens of millions of personal devices for unrelated reasons.
In a blog post, the company confirmed reports saying that new scanning technology is part of a suite of child protection programs that would “evolve and expand.” It will be rolled out as part of iOS 15, which is scheduled for release sometime in August.
Apple, which has often touted itself as a company that promises to safeguard users’ right to privacy, appeared to try and preempt privacy concerns by saying that the software will enhance those protections by avoiding the need to carry out widespread image scanning on its cloud servers.
“This innovative new technology allows Apple to provide valuable and actionable information to [the National Center for Missing and Exploited Children] and law enforcement regarding the proliferation of known CSAM,” said the company, referring to an acronym for child sexual abuse material. “And it does so while providing significant privacy benefits over existing techniques since Apple only learns about users’ photos if they have a collection of known CSAM in their iCloud Photos account. Even in these cases, Apple only learns about images that match known CSAM.”
The Cupertino-based tech giant said the system will utilize breakthrough cryptography technology and artificial intelligence to find abuse material when it is stored in iCloud Photos, said the firm in its blog post. The images will be matched to a known database of illegal images, the firm said, adding that if a certain number of those images are uploaded to iCloud Photos, the company will review them.
Those images—if they’re deemed illegal—will be reported to the National Center for Missing and Exploited Children. The software won’t be applied to videos, Apple added.
“Apple’s expanded protection for children is a game-changer,” John Clark, the president and CEO of the National Center for Missing and Exploited Children, said in a statement on Thursday about the initiative. “The reality is that privacy and child protection can coexist.“
But some security experts and researchers, who stressed they support efforts to combat child abuse, said the program could present significant privacy concerns.
When news of the proposal broke on Wednesday evening, John Hopkins University professor and cryptographer Matthew Green echoed those concerns.
Microsoft created PhotoDNA to assist companies in identifying child sexual abuse images on the internet, while Facebook and Google have implemented systems to flag and review possibly illegal content.