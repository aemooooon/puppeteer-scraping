Twitter CEO Elon Musk has joined dozens of artificial intelligence (AI) experts and industry executives in signing an open letter calling on all AI labs to “immediately pause” training of systems more powerful than Chat GPT-4 for at least six months.
They argue that AI systems with human-competitive intelligence can pose “profound risks to society and humanity,” and change the “history of life on Earth,” citing extensive research on the issue and acknowledgments by “top AI labs.”
Experts go on to state that there is currently limited planning and management regarding Advanced AI systems despite companies in recent months being “locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one—not even their creators—can understand, predict, or reliably control.”
“Contemporary AI systems are now becoming human-competitive at general tasks and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete, and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders,” the letter states.
The letter then calls for a public and verifiable minimum six-month pause on the training of AI systems more powerful than GPT-4 or a government-issued moratorium on such training if the pause cannot be enacted quickly.
During such a pause, AI labs and independent experts should use the time to create and implement a set of shared safety protocols for advanced AI design and development that are “rigorously audited” and overseen by independent third-party experts, the letter states.
Such protocols should be designed to make sure that systems adhering to them are safe beyond a reasonable doubt, accurate, trustworthy, and aligned, experts said.
Additionally, the letter calls on policymakers to swiftly develop robust AI governance systems such as authorities who are able to provide oversight and track highly capable AI systems, raise funding for additional AI safety research, and establish institutions that can cope with what they say will be the “dramatic economic and political disruptions (especially to democracy) that AI will cause.”
“This does not mean a pause on AI development in general, merely a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities,” experts noted in their letter.
“We think public standards about when an AGI effort should stop a training run, decide a model is safe to release, or pull a model from production use are important. Finally, we think it’s important that major world governments have insight about training runs above a certain scale,” the company said.
Further concerns have been raised over the software, which is trained using reinforcement learning from human feedback (RLHF), particularly regarding how it can be used to help students cheat on their exams and homework.