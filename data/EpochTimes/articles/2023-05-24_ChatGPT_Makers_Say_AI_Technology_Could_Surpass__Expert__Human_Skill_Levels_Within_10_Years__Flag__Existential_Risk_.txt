The makers of ChatGPT have warned that Artificial Intelligence (AI) could “exceed expert skill level” across most domains within the next 10 years as “superintelligence” becomes more powerful than any “other technologies humanity has had to contend with.”
The executives noted that it is “conceivable” that within the next ten years, AI systems could carry out as much productivity as one of today’s largest corporations.
“In terms of both potential upsides and downsides, superintelligence will be more powerful than other technologies humanity has had to contend with in the past,” they wrote. “We can have a dramatically more prosperous future but we have to manage risk to get there. Given the possibility of existential risk, we can’t just be reactive.”
The executives went on to lay out three proposals to address the risks associated with the increasingly widespread use and advancement of AI, including coordination across AI development leaders to ensure that the technology grows “in a manner that allows us to both maintain safety and help smooth integration of these systems with society.”
As part of this effort, major governments around the world could establish a growth rate under which AI capability is limited each year, the executives said.
The senior experts also noted that individual companies should be held to an“ extremely high standard” when it comes to acting responsibly.
Elsewhere, the Open AI executives suggested the creation of an agency like the International Atomic Energy Agency (IAEA) to oversee superintelligence efforts associated with the advancement of AI “above a certain capability.”
Such advanced AI would be subject to audits, inspections, and testing to ensure compliance with safety standards.
“It would be important that such an agency focus on reducing existential risk and not issues that should be left to individual countries, such as defining what an AI should be allowed to say,” the execs wrote.
Finally, they stated that technical capability is needed to ensure that superintelligence is safe.
“This is an open research question that we and others are putting a lot of effort into,” they wrote in the blog post.
Altman and his colleagues noted, however, that it would be “unintuitively risky and difficult” to prevent the creation of superintelligence, citing “tremendous upsides” to such technology as well as the difficulties in preventing its development, which they said would require something akin to a “global surveillance regime” that may not actually work.
They also stressed the importance of allowing companies and open-source projects to develop models below a significant capability threshold, without the kind of regulation they recommend in their blog post.
“Today’s systems will create tremendous value in the world and, while they do have risks, the level of those risks feel commensurate with other Internet technologies and society’s likely approaches seem appropriate,” they wrote. “By contrast, the systems we are concerned about will have power beyond any technology yet created, and we should be careful not to water down the focus on them by applying similar standards to technology far below this bar.”
Despite listing the various benefits of ChatGPT, Altman also told lawmakers that rapidly advancing AI technology will ultimately lead to widespread layoffs across various sectors.
On Monday, Microsoft co-founder Bill Gates also warned that AI developers are currently competing to develop the first personalized AI agents which could render retail giants such as Amazon useless, as they change the way consumers do everything from using search engines to shopping online.