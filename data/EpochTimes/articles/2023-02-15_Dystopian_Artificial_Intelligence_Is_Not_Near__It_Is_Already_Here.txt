In November 2022, the release of an artificial intelligence (AI) online chatting program named ChatGPT shocked the world. This program is so “smart” that it delivers frighteningly human-like responses and seems to have very few flaws compared to previous versions. Not only do people treat it as a conversation companion, but they have also started to use this AI technology for a variety of tasks, such as completing homework, creating stunning images, writing poems, etc.
ChatGPT stands for Chat Generative Pre-trained Transformer. The program is based on a set of technologies developed and used in programs that generate user-friendly responses. AI is something that needs to be taught how to think and respond using technology that functions like neural networks. This process involves feeding the AI mountains of information before it can process it, adapt, learn, create, and become intelligent.
ChatGPT can also do the same with essays, entire chunks of code, and much more. It can have a casual conversation with you and knows how to adjust its jargon to even explain quantum computing in a way seventh graders can understand it.
Pro-AI arguments see technology like ChatGPT as the next step in human advancement. It would make even science more efficient, reduce human labor, and make life easier.
The other side of the argument is that there is no way to hold artificial intelligence accountable for its work. If the program reaches the wrong conclusions or its algorithms aren’t mature enough, how can the program take responsibility for it?
The accountability issue is not just about when things go wrong. The use of AI-generated text without proper citation “could be considered plagiarism,” says Holden Thorp, editor-in-chief of the family of Science journals. For that reason, a few articles have already been published with ChatGPT listed as one of the authors, while publishers are hastening the push for regulation.
In fact, after papers were published in Nature with ChatGPT as a co-author, the editors-in-chief for Nature and Science concluded that “ChatGPT doesn’t meet the standard for authorship” because such a title carries accountability and liability to it, something out of the question for AI.
However, the core issue behind the authorship dispute is that journal editors are no longer certain about how much or to what extent the article was generated by ChatGPT. Scientific experiments likely still require studies conducted by humans. But authors of review articles that attribute ChatGPT likely did so because it played a significant role in the writing process.
Some biomedical researchers have used ChatGPT to conduct drug development research and have been able to identify potential drug chemicals that were missed in the past. With the help of AI, a new age of explosive advancements in the biomedical field is sure to be ushered in.
AI could become the next nurse or physician’s assistant that helps you recover after an accident, or that performs the key incisions on your next operation. The future of health care could transform rapidly, as people might not even have to go to the doctor’s office at all with the combination of AI and telemedicine. All you have to do is open an app on your phone and talk with a chatbot, tell it about your symptoms, and it will curate a prescription for you. But there is a level of trust developed during face-to-face interactions that is missing from this AI model.
AI robots using a GPT can also be used to treat high-risk patients such as those with mental disorders or in rehab by replacing the doctor when monitoring the patients and administering treatment, conducting checkups, evaluating risks, and taking action if needed. However, the same accountability question arises when we implement AI into the medical field.
Here, the accountability question is more concerning, because who will be held accountable when the patient experiences complications from the wrong medicine or the wrong dose? You can’t blame the doctor because he was just following the AI. You can’t blame the AI because it’s a program. In the end, who will be held accountable?
For people to feel safe around AI, strict liability rules need to be imposed to restrict the freedom these things have. However, if these programs are to improve, they need to have more freedom to operate and learn. Although this appears to be a catch-22, the core issue is whether humans should let AI and robots take care of them.
If AI becomes ubiquitous, will it make humans dumber and reduce us in all aspects? Over time, children might just talk to their chatbot tablets instead of their parents, people might forget how to alleviate symptoms of things as common as colds, and basic tasks like writing an essay might become things of the past. This will inevitably undermine humans and affect our development. When technology becomes so advanced that we can command robots with our minds, might we one day devolve into those aliens with lanky limbs and inflated heads?
When AI begins to mimic human thinking and presents human-like language, we begin to see the reality of the human brain laid bare: They are essentially machines that process information. When computers gather enough of a volume of data, they can engage a sophisticated algorithm to generate human-like thinking and response. The more people use it, the more the ChatGPT AI will be trained to become more human-like, possibly eventually becoming wiser than mankind.
So what makes us humans unique?
We have witnessed supercomputers defeat the human champions of chess and Go games.
Now, AI has arrived in the fields of which people are genuinely proud—fields that revolve around creation, emotion, human interaction, artistic expression, and so on.
This is a critical time when human beings need to think more deeply about where our wisdom comes from. Are our inspirations simply born of an accumulation of myriad data? AI and computers get their data from human input or via trawling the depths of seas of data. Do we, too, get our “original” ideas this way? Why do people get inspiration and creative ideas that seemingly have nothing to do with their prior experience and knowledge?
The threat of AI and supercomputers is not just about losing more jobs. And it goes beyond reducing human thinking capability. The fundamental threat of uncontrolled AI technology is that it cuts off human beings’ connection with our creator. Through technological advancement, human beings are constructing digital gods for people to worship. Using AI or robots to improve life may be the sweet side of this drug, but using AI to replace human thinking is the darker side.
The pressing issue here is how to safeguard our human spirituality. How do we maintain our connection to the divine? Human beings are not just flesh and bones, like how a machine is simply composed of mechanical parts.