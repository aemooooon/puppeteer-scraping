Imagine a self-driving car spotting a vehicle with a bicycle on top of it. It has detected an object, yes, but is it a bike or a car? Is it both? And most importantly, how should the system behave in response to it? This is where human help becomes indispensable, since only people can train computer vision models (the self-driving car’s “brain”) to properly detect complicated objects. They are also the ones fine-tuning AIs so that the latter understand different landscapes and avoid issues. Yandex self-driving cars, for instance, needed to label additional images from Las Vegas roads, such as traffic lights, which might be blinking yellow, unlike traffic lights in other countries. This way, Yandex helps its cars “wrap their head around” the town, one with little resemblance to Moscow, where they’d been originally “trained.”
Data labeling types for self-driving cars include segmentation, 2D bounding boxes, lane marking, video tracking annotation, point annotation and 3D object recognition—each requiring careful treatment, as together they teach AI to understand what’s happening on the road. The more often data is labeled accurately, the faster the AI will pick it up, produce patterns, and avoid generating a distorted picture, which impedes them from differentiating safe from unsafe situations.