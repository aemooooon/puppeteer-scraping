Our smartest machines look nothing like we predicted – has the field lost its way, or do we need to rethink what AI actually means, asks Tom Chatfield.
Would modern artificial intelligence live up to the dreams of the field’s founders? Perhaps not. But in many ways, the smartest machines we have built are entities they never could have imagined.
In 1956, attendees of a research camp at Dartmouth College in New Hampshire coined the phrase "artificial intelligence" to describe its efforts to “find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.”
Compare that with the AI project that Facebook announced this month. Under one of the world’s most prominent experts, it will “do world-class artificial-intelligence research using all of the knowledge that people have shared on Facebook” – with potential gains including building “services that are much more natural to interact with.”
What does that mean? Like much of modern AI, they will be training algorithms to sift and analyse unimaginably vast amounts of data in the hope that smart answers will emerge. Google, IBM and many others are now using this technique – called machine learning – to great commercial success, and the “intelligence” they create underpins everything from your internet searches to online language translation. Since the calculations of these machines involves making statistical correlations within huge caches of data, their reasoning can be unfathomable to the human mind – often these systems provide apparently intelligent answers, but nobody has any idea how they came to their conclusions.
Yet even the most advanced forms of machine intelligence cannot hope to pass for a human in Turing’s famous test – let alone use natural language or develop concepts themselves, as the pioneers hoped. More than half a century of research has brought us a far more sophisticated grasp of what machine intelligence looks like – but has it lost its way? Or do we need to reframe our ideas about what the term AI actually means?
One person who believes progress in AI has fallen short in many ways is the author and academic Douglas Hoftstadter – most famous for his Pulitzer-Prize-winning 1979 book Gödel, Escher, Bach – who in a recent profile for The Atlantic magazine emphasized his disillusionment with the current direction of AI.
For Hoftstadter, the label “intelligence” is simply inappropriate for describing insights drawn by brute computing power from massive data sets – because, from his perspective, the fact that results appear smart is irrelevant if the process underlying them bears no resemblance to intelligent thought. As he put it to interviewer James Somers, “I don’t want to be involved in passing off some fancy program’s behaviour for intelligence when I know that it has nothing to do with intelligence. And I don’t know why more people aren’t that way.”
Cheap tricks
By Hofstadter’s standards, iconic computational achievements like beating the world’s best players at chess or Jeopardy are rendered trivial by the “trickery” involved: by the fact that the winning computer has done little more than weigh the relative benefits of several billion possibilities, without at any point knowing anything about the nature of the game being played.
What computers scientists should be researching, he argues, is how the phenomenon of intelligence itself arises from the human brain – a question that those focused on data-led machine analysis (and its profitable applications) have increasingly side-lined.
How far current AI research can take us is open for debate, but even its proponents acknowledge that the current emphasis on data and pragmatism may only take us so far. In an analogy quoted by The Atlantic from a leading textbook in the field, it’s like trying to climb a tree to the moon. Progress is excellent at first – but cannot continue past a certain point. Years of success may remain in feeding vast quantities of problems and solutions into machines running clever learning programs, and watching them train themselves to produce good-enough outcomes. But what next, when that particular tree runs out? And where is it we should be aiming for in the first place?
Despite Hoftstadter’s protestations, perhaps we need to stop comparing machines with ourselves altogether. After all, while humans are the only examples we possess of phenomena like advanced language and logic, they’re hardly the earth’s only examples of intelligence. From pet parrots to ant colonies, our world is packed with “intelligent” responses.
How do we do that? A first step may be to change the language we use to describe these machine minds. Today, the very phrase “artificial intelligence” conjures certain iconic images; a robotic reflection of ourselves. Yet this hypothetical being stands amid an ocean of vagueness. From the phenomenon of consciousness to what it might mean to measure or analyse intelligence, our own minds remain profoundly mysterious – and those insights we do have are rooted in millennia of evolutionary history, the intricacies of our synapse-packed brains, and investigations spanning every field from literature to the social sciences.
Why should our biological manner of thinking determine our approach to silicon-based circuits and electronic logic? Our machine creations are more profoundly divided from us than anything else in nature. They do not need to think like us to serve us, work with us, or even understand us – as our own relationships with nature should teach us at a glance.
So what new words might we use in place of artificial intelligence? “Artificial” implies something bogus, ersatz and somehow unreal – which is why I feel the term “machine” fits better. “Intelligence” implies discernment and apprehension, but also something inexorably human – which is why I prefer the more impassive “reasoning.” Machine reasoning: it’s not a phrase for the ages, but perhaps a beginning to the process of seeing what waits under our noses.