Here’s a game to play over dinner. One person names a profession that they believe can’t be taken over by a machine, and another person has to make a case why it’s not so future-proof. We played this game on an upcoming episode of SBS’s Insight on the topic of the future of robots and artificial intelligence.
The first profession suggested was musician. An argument often put forwards against artificial intelligence (AI) is that computers can’t be creative. But there are plenty of examples to counter this argument. For instance, computers can take plain sheet music and turn it into an expressive jazz performance, as my colleague Ramon Lopez de Mantaras has shown.
So, jazz musicians watch out. Your jobs might not be safe from robot incursion.
The next option was police officer. It’s often said that computers can’t or won’t behave ethically. Unfortunately, Hollywood has already painted a very dystopian picture here in movies like Robocop and Terminator. And, as the current UN campaign to ban autonomous weapons demonstrates, we could easily end up there if we aren’t careful.
The third profession put forward was human resources. Naturally, this came from an HR consultant worried for her future job prospects. However, the bureaucratic side of HR is already easily automated. Indeed, we spend much of our lives on the phone already talking to machines. Can I speak to a real person, please?
On the other hand, the more human-facing side of HR is likely to be harder to automate. But as we argue in the next answer, it’s not clear that this will be impossible.
The fourth challenge was psychiatrist. Again, the human-facing nature of this would seem to offer significant resistance to automation. Nevertheless, there’s an interesting historical precedent.
A well known computer program called Eliza was the very first chatterbot. It unintentionally passed itself off as a real Rogerian psychotherapist.
Eliza was not very smart. Indeed, the program’s author, Joseph Weizenbaum, meant it more as parody than as therapist. However, his secretary famously asked
to be left alone so she could talk in private to the chatterbot.
So, shrinks watch out. Your jobs might not be safe.
The final challenge was Prime Minister.
On the one hand, this is a good answer, as one assumes there’s little routine to being Prime Minister but a lot of tough high level decision making that would be tough for a machine to handle. On the other hand, it’s a poor winner of our little game. It may be the only job in the whole country that’s safe from robots.
In one final, beautiful irony, this forthcoming episode of Insight has the robots up on the stage. We, the supposed expert commentators were in the audience. So, even TV pundits should watch out. Your jobs might not be safe too.
What this discussion highlights is that the middle classes are likely to be increasingly squeezed by machine labour. Professions that we used to think were quite safe – like doctor, lawyer or accountant – will be increasingly automated.
Whenever technology takes away jobs, it tends to make new jobs and industries elsewhere. For example, printing removed the need for scribes but created the vast publishing industry in its stead. And publishing went on to create many other jobs in the industries that grew out of all the knowledge passed on in printed material.
More recently, computers have taken away many traditional jobs in the printing industry, like type setters. But we now see many new jobs in areas like self-publishing and web design.
Economists continue to argue over the net effects of technology. Does technology create more economic activity so we are all better off? Or does it put more people out of work, concentrating wealth in the hands of the few?
One thing seems sure. It requires us to adapt. And for this, we need an educated, high tech workforce. This brings the conversation back to higher education and the stalled reforms that now trouble this sector in Australia.
If there is one policy we need to get right, to future-proof Australia against machines and other disruptions, I would argue, this is it.