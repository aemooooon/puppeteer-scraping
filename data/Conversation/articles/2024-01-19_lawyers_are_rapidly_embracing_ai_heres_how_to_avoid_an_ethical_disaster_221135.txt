Imagine a world where legal research is conducted by lightning-fast algorithms, mountains of contracts are reviewed in minutes and legal briefs are drafted with the eloquence of Shakespeare. This is the future promised by AI in legal practice. Indeed, AI tools are transforming this landscape already, venturing from science fiction into the everyday realities of lawyers and legal professionals.
However, this advancement raises ethical and regulatory concerns that threaten the very foundation of the justice system. At a time when the Post Office Horizon scandal has shown how a trusted institution can quickly wreck its reputation after introducing an opaque algorithmic system, it’s important to anticipate potential pitfalls and address them in advance.
We’ve already seen generative AI used at the highest levels of the profession. Lord Justice Birss, deputy head of civil justice in England and Wales, disclosed a few months ago that he had used ChatGPT to summarise an area of law, then incorporated it into his judgment. This marked the first instance of a British judge using an AI chatbot – and it’s just the tip of the iceberg.
For instance, I’m aware of one colleague, a property lawyer, using an AI contract analysis tool to discover a hidden clause in a land dispute case. I also know a lawyer who was facing an overwhelming amount of evidence in an environmental law claim and used an AI-powered document review. It sifted through thousands of documents, pinpointing key evidence that ultimately secured a substantial settlement for the client.
To date, the fastest adopters of generative AI in the legal profession have been lawyers working in-house for large companies, with 17% using the technology, according to legal analytics giant LexisNexis. Law firms are not far behind, with around 12% to 13% using the technology. In-house teams may be ahead because they are more motivated to save costs.
But large law firms look set to catch up on in-house legal teams, with around 64% actively exploring this technology, compared to 47% of in-house teams and around 33% of smaller legal practices. In future, large law firms may specialise in specific AI tools or develop in-house expertise, offering these services as a competitive advantage.
The vast majority of lawyers think this technology will have a discernible effect, according to a 2023 LexisNexis survey of over 1,000 UK legal professionals. Of these, 38% said it would be “significant”, while another 11% said “transformative”. Most respondents (67%) thought there would be a mixture of positive and negative effects, however, compared to only 14% who were wholly positive and 8% who were more negative.
Here are some examples of what is arriving.
Bail and sentencing: tools such as Compas and Equivant are now employing AI to help practitioners with these decisions.
These advancements hold immense potential for improving efficiency, reducing costs and democratising access to legal services. So what about the challenges?
AI algorithms are trained on datasets, which can reflect and amplify societal biases. For example, if a city has a history of over-policing certain neighbourhoods, an algorithm may recommend higher bail amounts for defendants from those areas, regardless of the risk of flight or re-offending.
Similar biases could affect the use of AI to hire lawyers within firms. There is also the potential for skewed results from the tools for legal research, document review and case prediction.
Equally, it can be difficult to understand how an AI arrived at a particular conclusion. This could undermine trust in lawyers and raise concerns about accountability. At the same time, over-reliance on AI tools might undermine lawyers’ own professional judgment and critical thinking skills.
Without proper regulations and oversight, there is also a risk of misuse and manipulation of these tools, jeopardising the fundamental principles of justice. In trials, for example, skewed training data may disadvantage trial participants based on factors unrelated to the case.
Here’s how we should address these issues.
1. Bias
We can mitigate by training AI models on datasets that represent the diversity of society, including race, gender, socioeconomic status and geographical location. Frequent and systematic audits of AI algorithms and models should also be conducted to detect biases.
AI developers such as OpenAI are already taking such steps, but it’s very much a work in progress and the results need to be monitored carefully.
2. Transparency
Developers such as IBM are devising a class of techniques and technologies known as explainable AI (XAI) tools to demystify the decision-making processes of AI algorithms. These need to be used to develop transparency reports for individual tools.
Full transparency on every neural connection may be unrealistic, but things like data sources and the AI’s general functionalities need to be visible.
3. Regulations and oversight
Clear legal guidelines are essential. This should include prohibiting AI tools trained on biased data, mandating transparency and traceability of data sources and algorithms, and establishing independent oversight bodies to audit and assess AI tools.
Ethics committees could provide additional oversight for the legal profession. These could be entirely independent, though they would be better established and overseen by a body like the Solicitors Regulation Authority.
In short, the rise of AI in legal practice is inevitable. Ultimately, the goal is not to replace lawyers with robots but to empower legal professionals so that they can focus more on the human aspects of law: empathy, advocacy and pursuing justice. It is time to ensure this transformative technology serves as a force for good, upholding the pillars of justice and fairness in the digital age.