Imagine the year is 2029. You have been living at the same address for a decade. The postman, who knows you well, smiles as he walks to your door and hands you a bunch of letters. As you sift through them, one card grabs your attention. It says: “Let us know if you are still here.”
It’s an election year and the card from the electoral office is asking you to confirm you are still a resident at the same address. It has a deadline, and you may be purged from the voter list if you don’t respond to it.
You had read about the government using AI to detect and eliminate electoral fraud through selective querying. Is it the AI pointing fingers at you? A quick check reveals your neighbours haven’t received any such cards. You feel singled out and insecure. Why have you been asked to prove that you live where you’ve lived for so long?
Let’s look under the hood. You received the card because election officials had deployed an AI system that can triangulate evidence to estimate why some voters should be contacted to check whether they are still a resident at their address. It profiles voters based on whether they display the behaviour of a “typical” resident.
In this case, you had taken early retirement and not filed tax returns in the past few years. And you had been on vacation during the previous election in 2024. These actions led the AI to conclude that you could be lingering in the electoral list illegitimately and triggered the system to contact you.
This fictional story is more plausible than you might think. In 2017 and 2018, more than 340,000 Wisconsin residents received a letter asking them to confirm if they needed to remain on the voter list. This was at the behest of a US-wide organisation called Eric, which had classified these voters as “movers”  – those who may have ceased to be residents. Eric used data on voting history to identify movers – but also administrative data such as driving licence and post office records.
Eric may not have used any sophisticated AI, but the logic it employed is very much the kind of logic that an AI would be expected to apply, only at a much larger scale.
The approach seemed highly effective. Only 2% of people responded, suggesting the vast majority of the people contacted were indeed movers. But research later showed systematic demographic patterns among Eric errors. The people erroneously identified as movers (and ended up showing up to vote) were far more likely to be from ethnic minorities.
AI algorithms are used in a variety of real-world settings to make judgments on human users. Supermarkets routinely use algorithms to judge whether you are a beer person or a wine person to send you targeted offers.
Every online payment transaction is being assessed by an AI in real-time to decide whether it could be fraudulent. If you’ve ever tried to buy something and ended up triggering an additional security measure – be it a password prompt or request for authentication on a mobile app – your bank’s AI was judging your attempted transaction as abnormal or suspect.
Our research shows that abundant AI capacity is available to make judgments on whether people’s behaviour is deviant or abnormal. To return to our opening example, in a world where early retirement is not the norm, an early retiree has the scales tipped against them.
Such social sorting, carried out by AI-based judgments, could be interpreted as a latent or soft form of majoritarian gerrymandering. Traditional gerrymandering is the unethical practice of redrawing electoral district boundaries to skew electoral outcomes. AI-based social sorting could disenfranchise people for behaving in a way that deviates from the way the majority behaves.
The patterns in the Wisconsin case should have us concerned that voters from ethnic minorities were systematically being classified as deviating from cultural norms.
In an ideal world, the electoral roll would include all eligible voters and exclude all ineligible voters. Clean voter lists are vital for democracy.
Having ineligible voters lurking on lists opens the possibility for spurious voting, skewing the result and damaging electoral integrity. On the other hand, leaving eligible voters off a list disenfranchises them and could result in election results that don’t reflect the true will of the people.
Ensuring access to the franchise to every eligible voter is therefore very important. To do a good job, efforts towards clean voter lists need to spread their focus reasonably between integrity and access.
The question, therefore, becomes whether AI is capable of doing this. As it stands today, AI is fundamentally a data-driven technology – one that is adept at looking at existing data and identifying regularities or irregularities.
It is much better equipped to spot issues with existing data than to identify instances of missing data. That means it is good at identifying people who may have moved from their registered address but not good at identifying new residents who have not registered to vote.
In a world of AI-driven electoral cleansing, you are much more likely to receive a “are you still here?” card than your new neighbour is likely to receive a “have you considered registering to vote?” card.
What this means for using AI to clean up voter lists is stark. It risks skewing the balance towards checking for integrity and away from enabling access. Integrity focused efforts in essence involve pointing fingers at people and putting the onus on them to confirm they are legitimate voters. Access focused efforts are like a welcoming pat on the back – an invitation to be part of the political process.
Even if widespread disenfranchisement doesn’t happen, states still risk undermining trust in elections by using AI on a larger scale. It could lead voters to feel electoral offices are obsessively oriented towards fault-finding and much less interested in democratic inclusion. And at a time when trust in elections is needed more than ever, that perception could be just as damaging as actually cutting people from electoral rolls.